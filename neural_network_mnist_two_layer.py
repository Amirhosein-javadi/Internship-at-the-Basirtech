# -*- coding: utf-8 -*-
"""neural_network_Mnist_two_layer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_IF9mwAZiP0V8MeW6hrnJNdu97xexYDP
"""

import tensorflow as tf
tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0],True)
import numpy as np
from keras import initializers,Sequential
from keras.layers import Input, Dense, Activation,BatchNormalization, Flatten, Conv2D, MaxPooling2D
from keras.models import Model
from keras.preprocessing import image
from keras import metrics
from keras.metrics import SparseTopKCategoricalAccuracy 
import keras.backend as K
K.set_image_data_format("channels_last")
import glob 
import matplotlib.pyplot as plt
import cv2
from sklearn.decomposition import PCA
from keras.datasets import mnist

def Create_Histogam(data):
  histogram = np.zeros(10)
  for i in range(10):
    histogram[i] = np.sum(data==i)
  return histogram

def Create_data_label(data,label,dim,start,stop):
  # one hot encoding for labels
  New_data = np.zeros((1,dim),dtype=data.dtype)
  New_label = np.zeros((1,10),dtype=label.dtype)
  for i in range(10):
      sample = np.where(label==i)[0][start:stop]
      d = data[sample]
      l = np.zeros((stop-start,10),dtype=label.dtype)
      l[:,i] = 1
      New_label = cv2.vconcat([New_label,l])
      New_data = cv2.vconcat([New_data,d])
  New_data = np.delete(New_data,0,0)
  New_label = np.delete(New_label,0,0)
  return New_data,New_label

(TrainX, TrainY), (TestX, TestY) = mnist.load_data()
train_histogram = Create_Histogam(TrainY)
test_histogram = Create_Histogam(TestY)
#change each image to a vector
train_samples = len(TrainX)
trainX = TrainX.reshape((train_samples,-1)).astype(np.float64)
test_samples = len(TestX)
testX = TestX.reshape((test_samples,-1)).astype(np.float64)
dim = (trainX.shape)[1]
# normalize each vector to 0 mean and 1 std
for i in range(train_samples):
  trainX[i] = (trainX[i]-np.mean(trainX[i]))/np.std(trainX[i])
for i in range(test_samples):
  testX[i] = (testX[i]-np.mean(testX[i]))/np.std(testX[i])

"""Fitting PCA to train vector and applying it to test and train vectors"""

variance = np.zeros((dim-1))
pca = PCA(n_components=dim)
pca.fit(trainX)
var = pca.explained_variance_ratio_
variance[0] = var[0] + var[1]
for i in range(1,dim-1):
  variance[i] = variance[i-1] + var[i+1]
n = np.where(variance>0.99)[0][0]
print(f'For having at least 0.99 variance, We need n_component of PCA to be {n}.')  
print("If you want to set a new n_component, press n.")
print("Otherwise, press any key.")
print("key: ")
word = input()
if word == 'n' or word == 'N':
  print("Enter the new n_component.")
  print("n_component: ")
  n = int(input())
pca = PCA(n_components=n)
pca.fit(trainX)
pca_trainX = pca.transform(trainX)
pca_testX = pca.transform(testX)

"""defining model"""

def My_model(n):
  tf.config.experimental.list_logical_devices('GPU')
  tf.debugging.set_log_device_placement(True)
  with tf.device(tf.test.gpu_device_name()):
      X_input = Input(shape=(n,))
      X = Dense(10, activation='softmax', name="Fc", kernel_initializer=initializers.GlorotUniform())(X_input)
      model = Model(inputs=X_input,outputs=X, name='My_Model')
      model.compile(optimizer="Adam", loss='categorical_crossentropy', metrics=['accuracy'])
      return model

"""Fitting and testing the model"""

train_X,train_Y = Create_data_label(pca_trainX,TrainY,n,0,int(np.min(train_histogram)))
test_X,test_Y = Create_data_label(pca_testX,TestY,n,0,int(np.min(test_histogram)))
with tf.device(tf.test.gpu_device_name()):
  model = My_model(n)
  model.fit(train_X,train_Y, epochs=40)
  loss,precision =  model.evaluate(train_X,train_Y)

"""Final result"""

print(f'The precision of network is {precision}.')
print(f'The loss of network is {loss}.')